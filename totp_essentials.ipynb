{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOOCV on DementiaBank "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was run using:\n",
    "- gluonnlp                  0.8.1                    pypi_0    pypi\n",
    "- mxnet-cu100mkl            1.5.0                    pypi_0    pypi\n",
    "- nltk                      3.4.5                    py37_0  \n",
    "- sacremoses                0.0.33                   pypi_0    pypi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T23:02:29.666852Z",
     "start_time": "2020-07-12T23:02:29.664341Z"
    }
   },
   "outputs": [],
   "source": [
    "#!mkdir totp_data/txtControl; mkdir totp_data/txtDementia\n",
    "#!cat totp_data/*.py\n",
    "#!python totp_data/chat2txt.py --chatdir 'totp_data/Control/*.cha' --output totp_data/txtControl\n",
    "#!python totp_data/chat2txt.py --chatdir 'totp_data/Dementia/*.cha' --output totp_data/txtDementia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T22:54:20.838615Z",
     "start_time": "2020-07-12T22:54:20.660927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the scene is in the in the kitchen \r\n",
      " the mother is wiping dishes and the water is running on the floor \r\n",
      " a child is trying to get a boy is trying to get cookies out of a jar and he's about to tip over on a stool \r\n",
      " the little girl is reacting to his falling \r\n",
      " it seems to be summer out \r\n",
      " the window is open \r\n",
      " the curtains are blowing \r\n",
      " it must be a gentle breeze \r\n",
      " there's grass outside in the garden \r\n",
      " mother's finished certain of the the dishes \r\n"
     ]
    }
   ],
   "source": [
    "!head totp_data/txtControl/002-0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T22:54:22.082287Z",
     "start_time": "2020-07-12T22:54:21.748443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n",
      "309\n"
     ]
    }
   ],
   "source": [
    "!ls totp_data/txtControl | wc -l\n",
    "!ls totp_data/txtDementia | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T22:54:23.154641Z",
     "start_time": "2020-07-12T22:54:23.130207Z"
    }
   },
   "outputs": [],
   "source": [
    "#http://gluon-nlp.mxnet.io/master/examples/language_model/language_model.html\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon.utils import download\n",
    "import gluonnlp as nlp\n",
    "import pandas as pd\n",
    "#Collect data\n",
    "_dir='./totp_data/'\n",
    "\n",
    "control_text = [open(filename).read() for filename in glob.glob(_dir+\"txtControl/*.txt\")]\n",
    "control_names = [filename.replace('./totp_data/txtControl/','') for filename in glob.glob(_dir+\"txtControl/*.txt\")]\n",
    "dementia_text =[open(filename).read() for filename in glob.glob(_dir+\"txtDementia/*.txt\")]\n",
    "dementia_names=[filename.replace('./totp_data/txtDementia/','') for filename in glob.glob(_dir+\"txtDementia/*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T22:54:24.178392Z",
     "start_time": "2020-07-12T22:54:24.170206Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "control_frame=pd.DataFrame(list(zip(control_names,control_text)),columns=['file','text'])\n",
    "dementia_frame=pd.DataFrame(list(zip(dementia_names,dementia_text)),columns=['file','text'])\n",
    "control_frame['label']=np.zeros(len(control_frame))\n",
    "dementia_frame['label']=np.ones(len(dementia_frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T22:54:24.928625Z",
     "start_time": "2020-07-12T22:54:24.920731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>021-1.txt</td>\n",
       "      <td>okay \\n many or the mother's washing the dish...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>208-1.txt</td>\n",
       "      <td>i see the lady drying the dishes \\n and the w...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>071-0.txt</td>\n",
       "      <td>you mean this house \\n oh whatever \\n uhhuh \\...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>678-0.txt</td>\n",
       "      <td>well the sink's overflowing \\n and the stool'...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>243-1.txt</td>\n",
       "      <td>i see a boy getting in the in the cookie jar ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file                                               text  label\n",
       "0  021-1.txt   okay \\n many or the mother's washing the dish...    0.0\n",
       "1  208-1.txt   i see the lady drying the dishes \\n and the w...    0.0\n",
       "2  071-0.txt   you mean this house \\n oh whatever \\n uhhuh \\...    0.0\n",
       "3  678-0.txt   well the sink's overflowing \\n and the stool'...    0.0\n",
       "4  243-1.txt   i see a boy getting in the in the cookie jar ...    0.0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T22:54:25.832008Z",
     "start_time": "2020-07-12T22:54:25.827987Z"
    }
   },
   "outputs": [],
   "source": [
    "conids=set()\n",
    "demids=set()\n",
    "for id in control_frame['file']:\n",
    "    conids.add(id[:3])\n",
    "for id in dementia_frame['file']:\n",
    "    demids.add(id[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T22:54:30.843475Z",
     "start_time": "2020-07-12T22:54:30.839439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participants:  99  controls and  194  cases (dementia)\n",
      "Transcripts :  242  controls and  309  cases (dementia)\n"
     ]
    }
   ],
   "source": [
    "print('Participants: ',len(conids),' controls and ',len(demids),' cases (dementia)')\n",
    "print('Transcripts : ',len(control_frame['file']),' controls and ',len(dementia_frame['file']),' cases (dementia)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T22:54:36.717229Z",
     "start_time": "2020-07-12T22:54:36.714218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" okay \\n the mother is washing dishes \\n and the water is flowing from the sink \\n she's looking out the window \\n the son or the boy's on a stool which he's about to fall off stealing cookies from the cookie jar and giving it to probably his little sister \\n there are two cups and a dish on the sink counter \\n and the lady at the sink has a dish and a wash and a dry cloth \\n do you want me to describe her clothes \\n oh \\n\""
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dementia_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T23:01:09.957317Z",
     "start_time": "2020-07-12T23:01:09.934668Z"
    }
   },
   "outputs": [],
   "source": [
    "#methods to train and evaluate the RNN - very minor modifications to standard GluonNLP routines\n",
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [detach(i) for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden\n",
    "\n",
    "# Note that ctx is short for context\n",
    "def mevaluate(model, data_source, batch_size, ctx):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(\n",
    "        batch_size=batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    for i, (data, target) in enumerate(data_source):\n",
    "        data = data.as_in_context(ctx)\n",
    "        target = target.as_in_context(ctx)\n",
    "        output, hidden = model(data, hidden)\n",
    "        hidden = detach(hidden)\n",
    "        L = loss(output.reshape(-3, -1), target.reshape(-1))\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        #ntotal += L.size - only count non-zero results so zero paddiing doesn't increase the denominator\n",
    "        ntotal += (L != 0).sum().asnumpy()[0]\n",
    "    if (ntotal > 0):\n",
    "        return total_L / ntotal\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Function for actually training the model\n",
    "def mtrain(model, train_data, val_data, test_data, epochs, lr):\n",
    "    best_val = float(\"Inf\")\n",
    "    start_train_time = time.time()\n",
    "    parameters = model.collect_params().values()\n",
    "\n",
    "    print('len train data ',len(train_data))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0.0\n",
    "        start_epoch_time = time.time()\n",
    "        start_log_interval_time = time.time()\n",
    "        hiddens = [model.begin_state(batch_size//len(context), func=mx.nd.zeros, ctx=ctx)\n",
    "                   for ctx in context]\n",
    "\n",
    "        for i, (data, target) in enumerate(train_data):\n",
    "            data_list = gluon.utils.split_and_load(data, context,\n",
    "                                                   batch_axis=1, even_split=True)\n",
    "            target_list = gluon.utils.split_and_load(target, context,\n",
    "                                                     batch_axis=1, even_split=True)\n",
    "            hiddens = detach(hiddens)\n",
    "            L = 0\n",
    "            Ls = []\n",
    "\n",
    "            with autograd.record():\n",
    "                for j, (X, y, h) in enumerate(zip(data_list, target_list, hiddens)):\n",
    "                    output, h = model(X, h)\n",
    "                    batch_L = loss(output.reshape(-3, -1), y.reshape(-1,))\n",
    "                    L = L + batch_L.as_in_context(context[0]) / (len(context) * X.size)\n",
    "                    Ls.append(batch_L / (len(context) * X.size))\n",
    "                    hiddens[j] = h\n",
    "            L.backward()\n",
    "            grads = [p.grad(x.context) for p in parameters for x in data_list]\n",
    "            gluon.utils.clip_global_norm(grads, grad_clip)\n",
    "\n",
    "            trainer.step(1)\n",
    "\n",
    "            total_L += sum([mx.nd.sum(l).asscalar() for l in Ls])\n",
    "\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                cur_L = total_L / log_interval\n",
    "                print('[Epoch %d Batch %d/%d] loss %.2f, ppl %.2f, '\n",
    "                      'throughput %.2f samples/s'%(\n",
    "                    epoch, i, len(train_data), cur_L, math.exp(cur_L),\n",
    "                    batch_size * log_interval / (time.time() - start_log_interval_time)))\n",
    "                total_L = 0.0\n",
    "                start_log_interval_time = time.time()\n",
    "\n",
    "        mx.nd.waitall()\n",
    "\n",
    "        print('[Epoch %d] throughput %.2f samples/s'%(\n",
    "                    epoch, len(train_data)*batch_size / (time.time() - start_epoch_time)))\n",
    "\n",
    "        val_L = mevaluate(model, val_data, batch_size, context[0])\n",
    "        print('[Epoch %d] time cost %.2fs, valid loss %.2f, valid ppl %.2f'%(\n",
    "            epoch, time.time()-start_epoch_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = mevaluate(model, test_data, batch_size, context[0])\n",
    "            model.save_parameters('{}_{}-{}.params'.format(model_name, dataset_name, epoch))\n",
    "            print('test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            lr = lr*0.25\n",
    "            print('Learning rate now %f'%(lr))\n",
    "            trainer.set_learning_rate(lr)\n",
    "\n",
    "    print('Total training throughput %.2f samples/s'%(\n",
    "                            (batch_size * len(train_data) * epochs) /\n",
    "                            (time.time() - start_train_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T23:00:29.507207Z",
     "start_time": "2020-07-12T23:00:25.766999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/cohenta/anaconda3/envs/gluon37:\r\n",
      "gluonnlp                  0.8.1                    pypi_0    pypi\r\n",
      "mxnet-cu100mkl            1.5.0                    pypi_0    pypi\r\n",
      "nltk                      3.4.5                    py37_0  \r\n",
      "sacremoses                0.0.33                   pypi_0    pypi\r\n"
     ]
    }
   ],
   "source": [
    "#versions\n",
    "!conda list | grep \"mxnet\\|gluon\\|nltk\\|moses\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T23:01:43.279817Z",
     "start_time": "2020-07-12T23:01:43.273571Z"
    }
   },
   "outputs": [],
   "source": [
    "#tokinzation / batchification \n",
    "def tokenize_frame(incoming_frame):\n",
    "    toks=[]\n",
    "    for response in incoming_frame['text']:\n",
    "        toks.extend(moses_tokenizer.tokenize(response.replace('\\n',' ')))    \n",
    "        toks.extend(['<eos>'])\n",
    "    return toks\n",
    "\n",
    "def tokenize_frames(train_frame,dementia_frame,test_frame):\n",
    "    toks=tokenize_frame(train_frame)\n",
    "    test_toks=tokenize_frame(test_frame)\n",
    "    dem_toks=tokenize_frame(dementia_frame)\n",
    "    return toks, dem_toks, test_toks\n",
    "        \n",
    "def batchify(trainset,valset,testset,dtrainset,dvalset):\n",
    "    adasets=[trainset,valset,testset,dtrainset,dvalset]\n",
    "    ad_train_data, ad_val_data, test_data,d_train_data,d_val_data = [bptt_batchify(dataset) for dataset in adasets]\n",
    "    return ad_train_data, ad_val_data, d_train_data,d_val_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T23:03:16.199212Z",
     "start_time": "2020-07-12T23:03:16.190497Z"
    }
   },
   "outputs": [],
   "source": [
    "bird_text = [open(filename).read().lower() for filename in glob.glob(\"totp_data/bird_transcripts/*.txt\")]\n",
    "bird_names = [filename.replace('totp_data/bird_transcripts/','') for filename in glob.glob(\"totp_data/bird_transcripts/*.txt\")]\n",
    "bird_frame=pd.DataFrame(list(zip(bird_names,bird_text)),columns=['file','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T23:04:13.520670Z",
     "start_time": "2020-07-12T23:04:13.518094Z"
    }
   },
   "outputs": [],
   "source": [
    "#to use development version of semvecpy - but could also just pip install semvecpy\n",
    "import sys\n",
    "#!git clone https://github.com/semanticvectors/semvecpy\n",
    "sys.path.append('semvecpy')\n",
    "import semvecpy.vectors.real_vectors as rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-12T23:06:08.083563Z",
     "start_time": "2020-07-12T23:06:07.739373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totp_data/wikipedia_sgns_embeddings.bin\r\n"
     ]
    }
   ],
   "source": [
    "!ls totp_data/wikipedia_sgns_embeddings.bin\n",
    "!mkdir totp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-12T23:17:50.893Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture gluout\n",
    "#load data\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "import math\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon.utils import download\n",
    "import gluonnlp as nlp\n",
    "from gluonnlp.model import train\n",
    "from sacremoses import MosesTokenizer\n",
    "moses_tokenizer = MosesTokenizer('en')\n",
    "\n",
    "dataset_name='DementiaBank'\n",
    "control_frame=pd.DataFrame(list(zip(control_names,control_text)),columns=['file','text'])\n",
    "dementia_frame=pd.DataFrame(list(zip(dementia_names,dementia_text)),columns=['file','text'])\n",
    "control_frame['label']=np.zeros(len(control_frame))\n",
    "dementia_frame['label']=np.ones(len(dementia_frame))\n",
    "double_frame=control_frame.append(dementia_frame)\n",
    "\n",
    "allids=[]\n",
    "\n",
    "for id in double_frame['file']:\n",
    "    allids.append(id[:3])\n",
    "\n",
    "double_frame['id']=allids\n",
    "\n",
    "#configure models\n",
    "num_gpus = 1\n",
    "context = [mx.gpu(i) for i in range(num_gpus)] if num_gpus else [mx.cpu()]\n",
    "log_interval = 200\n",
    "batch_size = 20 * len(context)\n",
    "test_batch_size = 1* len(context)\n",
    "lr = 20\n",
    "epochs = 20\n",
    "bptt = 10\n",
    "grad_clip = 0.25\n",
    "\n",
    "print('Participants: ',len(conids),' controls and ',len(demids),' cases (dementia)')\n",
    "print('Transcripts : ',len(dementia_frame['file']),' controls and ',len(control_frame['file']),' cases (dementia)')\n",
    "\n",
    "\n",
    "# Specify the loss function, in this case, cross-entropy with softmax.\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "#LOOCV\n",
    "#earp = 'random' #uncomment for stochastic initiliazation\n",
    "earp = 'basic'   #uncomment for pretrained embeddings\n",
    "#earp = 'wiki'   #uncomment for pretrained LM\n",
    "\n",
    "dataset_name = None\n",
    "pretrained = False\n",
    "\n",
    "for earp in ['random','basic','wiki']:\n",
    "\n",
    "    for e in [1]: #,2,3,4,5,6,7,8,9,10]:   #ten iterations used for paper\n",
    "        f = open('totp_output/'+str(e)+'gluon_output_'+earp+'.tmp', \"w\")\n",
    "        g = open('totp_output/'+str(e)+'bird_output_'+earp+'.tmp',\"w\")\n",
    "        sallids=set(allids)\n",
    "        prefix = 'totp_data/wikipedia_sgns_'\n",
    "        posfix = 'embeddings.bin'\n",
    "\n",
    "\n",
    "        for currentid in list(sallids):\n",
    "\n",
    "            if earp != 'random':\n",
    "                lr = 5\n",
    "                if earp == 'wiki':\n",
    "                    dataset_name = 'wikitext-2'\n",
    "                    pretrained = True\n",
    "                else:\n",
    "                    glovecs = rv.RealVectorStore()\n",
    "                    glovecs.init_from_file(prefix+earp+posfix)\n",
    "\n",
    "            test_frame=double_frame[double_frame['file'].str.contains('^'+currentid)]\n",
    "            train_frame=control_frame[~control_frame['file'].str.contains('^'+currentid)]\n",
    "            dem_frame=dementia_frame[~dementia_frame['file'].str.contains('^'+currentid)]\n",
    "\n",
    "            toks, dem_toks, test_toks = tokenize_frames(train_frame,dem_frame,test_frame)\n",
    "            x=int(.9*len(toks))\n",
    "            trainset=toks[:x]\n",
    "            valset=toks[x:]\n",
    "\n",
    "            x=int(.9*len(dem_toks))\n",
    "            dtrainset=dem_toks[:x]\n",
    "            dvalset=dem_toks[x:]\n",
    "\n",
    "            testset=[]\n",
    "            testset=test_toks\n",
    "\n",
    "            vocab = nlp.Vocab(counter=nlp.data.Counter(trainset+dtrainset))\n",
    "            dvocab = nlp.Vocab(counter=nlp.data.Counter(trainset+dtrainset))\n",
    "\n",
    "            bptt_batchify = nlp.data.batchify.CorpusBPTTBatchify(\n",
    "                vocab, bptt, batch_size, last_batch='keep')  #changed from 'discard'\n",
    "\n",
    "            dbptt_batchify = nlp.data.batchify.CorpusBPTTBatchify(\n",
    "                dvocab, bptt, batch_size, last_batch='keep')\n",
    "\n",
    "            train_data, val_data, test_data = [\n",
    "                bptt_batchify(x) for x in [trainset, valset, testset]]\n",
    "\n",
    "            dtrain_data, dval_data, dtest_data = [\n",
    "                dbptt_batchify(x) for x in [dtrainset, dvalset, testset]]\n",
    "\n",
    "            model_name = 'standard_lstm_lm_200'\n",
    "            dmodel_name = 'standard_lstm_lm_200'\n",
    "            model, vocab = nlp.model.get_model(model_name, vocab=vocab, dataset_name=dataset_name,pretrained=pretrained,ctx=context[0])\n",
    "            dmodel, dvocab = nlp.model.get_model(dmodel_name, vocab=dvocab, dataset_name=dataset_name,pretrained=pretrained,ctx=context[0])\n",
    "\n",
    "            # Initialize the model\n",
    "            if (dataset_name == None):\n",
    "                dmodel.initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "            #rebatchify if pretrained (use pretrained vocab)\n",
    "            if (earp == 'wiki'):\n",
    "                vocab.padding_token = '<pad>'\n",
    "                bptt_batchify = nlp.data.batchify.CorpusBPTTBatchify(\n",
    "                    vocab, bptt, batch_size, last_batch='keep')  #changed from 'discard'\n",
    "                dbptt_batchify = nlp.data.batchify.CorpusBPTTBatchify(\n",
    "                    vocab, bptt, batch_size, last_batch='keep')\n",
    "                train_data, val_data, test_data = [\n",
    "                    bptt_batchify(x) for x in [trainset, valset, testset]]\n",
    "                dtrain_data, dval_data, dtest_data = [\n",
    "                    dbptt_batchify(x) for x in [dtrainset, dvalset, testset]]\n",
    "                dvocab = vocab\n",
    "\n",
    "\n",
    "            # Replace embeddings with pretrained (not necessarily Glove) vectors\n",
    "            if earp != 'random' and earp != 'wiki':\n",
    "                    parameters = dmodel.collect_params().values()\n",
    "                    ps=[]\n",
    "                    for p in parameters:\n",
    "                        ps.append(p)\n",
    "\n",
    "                    print(len(ps[0].list_data()[0].asnumpy()), len(vocab))\n",
    "                    terms=[]\n",
    "                    vecs=[]\n",
    "                    for i in (range(len(vocab))):\n",
    "                        term = vocab.idx_to_token[i]\n",
    "                        terms.append(term)\n",
    "                        if term in glovecs.dict:\n",
    "                            vecs.append(glovecs.get_vector(term).copy().vector)\n",
    "                        else:\n",
    "                            vecs.append(ps[0].list_data()[0][i].asnumpy())\n",
    "                    ps[0].set_data(np.array(vecs))\n",
    "\n",
    "            # Initialize the trainer and optimizer and specify some hyperparameters\n",
    "            trainer = gluon.Trainer(dmodel.collect_params(), 'sgd', {\n",
    "                'learning_rate': lr,\n",
    "                'momentum': 0,\n",
    "                'wd': 0\n",
    "            })\n",
    "\n",
    "            #train dementia model\n",
    "            mtrain(\n",
    "                dmodel,\n",
    "                dtrain_data, \n",
    "                dval_data,\n",
    "                dtest_data, \n",
    "                epochs=epochs,\n",
    "                lr=lr)\n",
    "\n",
    "\n",
    "            if (dataset_name == None):\n",
    "                model.initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "            # Replace embeddings with pretrained Glove vectors\n",
    "            if earp != 'random' and earp != 'wiki':\n",
    "                    parameters = model.collect_params().values()\n",
    "                    ps=[]\n",
    "                    for p in parameters:\n",
    "                        ps.append(p)\n",
    "\n",
    "                    print(len(ps[0].list_data()[0].asnumpy()), len(vocab))\n",
    "                    terms=[]\n",
    "                    vecs=[]\n",
    "                    for i in (range(len(vocab))):\n",
    "                        term = vocab.idx_to_token[i]\n",
    "                        terms.append(term)\n",
    "                        if term in glovecs.dict:\n",
    "                            vecs.append(glovecs.get_vector(term).copy().vector)\n",
    "                        else:\n",
    "                            vecs.append(ps[0].list_data()[0][i].asnumpy())\n",
    "                    ps[0].set_data(np.array(vecs))\n",
    "\n",
    "\n",
    "            trainer = gluon.Trainer(model.collect_params(), 'sgd', {\n",
    "                'learning_rate': lr,\n",
    "                'momentum': 0,\n",
    "                'wd': 0\n",
    "            })\n",
    "\n",
    "            #train control model \n",
    "            mtrain(\n",
    "                model,\n",
    "                train_data, \n",
    "                val_data,\n",
    "                test_data, \n",
    "                epochs=epochs,\n",
    "                lr=lr)\n",
    "\n",
    "\n",
    "            mixframe = pd.DataFrame(columns=['xdementia','perplexity','file'])\n",
    "            cparameters = model.collect_params().values()\n",
    "            dparameters = dmodel.collect_params().values()\n",
    "            cps=[]\n",
    "            for p in cparameters:\n",
    "                cps.append(p)\n",
    "            dps=[]\n",
    "            for p in dparameters:\n",
    "                dps.append(p)\n",
    "            #get average of parameters (interrogation by interpolation)\n",
    "            mix_models = []\n",
    "            for alpha in [0,.25,.5,.75,1]:\n",
    "                model_name = 'standard_lstm_lm_200'\n",
    "                mix_model, vocab = nlp.model.get_model(model_name, vocab=vocab, dataset_name=None)\n",
    "                mix_model.initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "                mix_parameters = mix_model.collect_params().values()\n",
    "                for i,p in enumerate(mix_parameters):\n",
    "                    cp = cps[i].list_data()[0].asnumpy()\n",
    "                    dp = dps[i].list_data()[0].asnumpy()\n",
    "                    nudata = (alpha*dp + (1-alpha)*cp)\n",
    "                    p.set_data(np.asarray(nudata))\n",
    "\n",
    "                mix_models.append(mix_model)\n",
    "           \n",
    "            #output bird results (interrogation by perturbation)\n",
    "            for i,row in bird_frame.iterrows():\n",
    "                bird_doutstring = ''\n",
    "                tdoc_toks=[]\n",
    "                tdoc_toks.extend(moses_tokenizer.tokenize(row['text'].replace('\\n',' ')))    \n",
    "                tdoc_toks.extend(['<eos>'])\n",
    "                test_bptt_batchify = nlp.data.batchify.CorpusBPTTBatchify(\n",
    "                vocab, len(tdoc_toks), 1, last_batch='keep')  #changed from 'discard'\n",
    "                test_doc = test_bptt_batchify(tdoc_toks)\n",
    "                for mix_model in mix_models:\n",
    "                    dres = mevaluate(mix_model, test_doc, 1, context[0])\n",
    "                    bird_doutstring= bird_doutstring +str(math.exp(dres))+\",\"\n",
    "                print(row['file'],',',bird_doutstring[:-1],file=g)\n",
    "\n",
    "\n",
    "            for i,row in test_frame.iterrows():\n",
    "                doutstring = ''\n",
    "                tdoc_toks=[]\n",
    "                tdoc_toks.extend(moses_tokenizer.tokenize(row['text'].replace('\\n',' ')))    \n",
    "                tdoc_toks.extend(['<eos>'])\n",
    "\n",
    "                held_test_bptt = len(tdoc_toks)\n",
    "\n",
    "                test_bptt_batchify = nlp.data.batchify.CorpusBPTTBatchify(\n",
    "                vocab, held_test_bptt, test_batch_size, last_batch='keep')  #changed from 'discard'\n",
    "\n",
    "                test_dbptt_batchify = nlp.data.batchify.CorpusBPTTBatchify(\n",
    "                dvocab, held_test_bptt, test_batch_size, last_batch='keep')\n",
    "\n",
    "                test_doc = test_bptt_batchify(tdoc_toks)\n",
    "                \n",
    "                for mix_model in mix_models:\n",
    "                    dres = mevaluate(mix_model, test_doc, 1, context[0])\n",
    "                    doutstring = doutstring + str(math.exp(dres)) + ','\n",
    "                \n",
    "                print(row['label'],',',doutstring[:-1],',',row['file'],file=f)\n",
    "\n",
    "            f.flush()\n",
    "            g.flush()\n",
    "        f.close()\n",
    "        g.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
